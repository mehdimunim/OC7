{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf40dNMxcRg6"
   },
   "source": [
    "#  Notebook 1 : Analyse exploratoire et préparation des données\n",
    "\n",
    "**Auteur :** Mehdi MUNIM\n",
    "\n",
    "**Date :** 2023-10\n",
    "\n",
    "**Description :**\n",
    "\n",
    "Ce notebook porte sur l'analyse exploratoire et la préparation des données de tweets pour un projet de classification de sentiment. Il comprend les étapes suivantes :\n",
    "\n",
    "*   Chargement et aperçu des données\n",
    "*   Nettoyage des données\n",
    "*   Analyse exploratoire des données\n",
    "*   Préparation des données pour l'entraînement\n",
    "*   Sauvegarde des données prétraitées\n",
    "\n",
    "**Objectif :**\n",
    "\n",
    "L'objectif de ce notebook est de préparer les données pour l'entraînement de modèles de Machine Learning qui seront utilisés pour prédire le sentiment (positif ou négatif) de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: / ^C\n"
     ]
    }
   ],
   "source": [
    "!conda install --y scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpgNNHs4YsiD",
    "outputId": "0b1cead2-bdb1-48b6-823f-318f76b2792a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (/home/mehdi/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/scipy/linalg/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer  \u001b[38;5;66;03m# Importation de TfidfVectorizer\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Librairies pour les embeddings\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec, Doc2Vec\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggedDocument\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Librairies pour la sauvegarde des données\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/gensim/matutils.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/home/mehdi/miniconda3/envs/sentiment_analysis/lib/python3.12/site-packages/scipy/linalg/__init__.py)"
     ]
    }
   ],
   "source": [
    "#@title Importations\n",
    "\n",
    "# Librairies pour la manipulation et l'analyse des données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librairies pour le traitement du langage naturel (NLP)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re  # Importation de re\n",
    "\n",
    "# Librairies pour la visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librairies pour le Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Importation de TfidfVectorizer\n",
    "\n",
    "# Librairies pour les embeddings\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Librairies pour la sauvegarde des données\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Téléchargement des ressources NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2hHJneEbaRA",
    "outputId": "771b7a69-cb1a-42f6-a0de-0bdad89b1d0f"
   },
   "outputs": [],
   "source": [
    "#@title Montage du Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma79lE0Scxhc"
   },
   "source": [
    "## 1. Chargement et aperçu des données\n",
    "\n",
    "Dans cette section, nous chargeons les données de tweets à partir du fichier CSV `training.1600000.processed.noemoticon.csv`.\n",
    "\n",
    "**Le jeu de données contient les colonnes suivantes :**\n",
    "\n",
    "*   Sentiment (0 pour négatif, 4 pour positif)\n",
    "*   ID du tweet\n",
    "*   Date du tweet\n",
    "*   Nom d'utilisateur\n",
    "*   Texte du tweet\n",
    "\n",
    "Nous utilisons la librairie pandas pour lire le fichier CSV et afficher les premières lignes du DataFrame, les noms de colonnes, les types de données et les statistiques descriptives. Cela nous permet de vérifier que les données ont été chargées correctement et d'avoir un premier aperçu de leur structure et de leur contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dC6obw1gZZpR",
    "outputId": "be27e1e9-65a7-492e-f60f-dc8734be84b0"
   },
   "outputs": [],
   "source": [
    "#@title Chargement et aperçu des données\n",
    "\n",
    "# Téléchargement et décompression du fichier zip\n",
    "!wget \"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip\"\n",
    "!unzip sentiment140.zip\n",
    "\n",
    "# Chargement des données du fichier CSV\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "\n",
    "# Affichage des premières lignes du DataFrame\n",
    "print(\"Premières lignes du DataFrame :\")\n",
    "display(df.head())\n",
    "\n",
    "# Affichage des noms de colonnes\n",
    "print(\"\\nNoms de colonnes :\")\n",
    "print(df.columns)\n",
    "\n",
    "# Affichage des types de données et des informations manquantes\n",
    "print(\"\\nInformations sur le DataFrame :\")\n",
    "df.info()\n",
    "\n",
    "# Affichage des statistiques descriptives\n",
    "print(\"\\nStatistiques descriptives :\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNn9pqAecmUo"
   },
   "source": [
    "## 2. Nettoyage des données\n",
    "\n",
    "Dans cette section, nous effectuons un nettoyage des données textuelles des tweets afin de les préparer à l'analyse et à la modélisation. Ce nettoyage comprend les étapes suivantes :\n",
    "\n",
    "*   **Suppression des éléments non pertinents**:  Nous supprimons les mentions (@utilisateur), les hashtags (#mot-clé) et les liens (URL) car ils n'apportent généralement pas d'information utile pour l'analyse de sentiment.\n",
    "*   **Suppression des caractères spéciaux et de la ponctuation**:  Nous supprimons tous les caractères qui ne sont pas des lettres ou des espaces.\n",
    "*   **Conversion en minuscules**:  Nous convertissons tout le texte en minuscules pour uniformiser les données et éviter que les mêmes mots soient traités différemment en fonction de leur casse.\n",
    "*   **Suppression des mots vides (stop words)**:  Nous supprimons les mots très fréquents (\"the\", \"a\", \"is\", etc.) qui n'apportent pas de sens particulier à l'analyse.\n",
    "*   **Lemmatisation**:  Nous réduisons les mots à leur forme canonique (lemme) pour regrouper les différentes formes fléchies d'un même mot (ex: \"running\" devient \"run\").\n",
    "\n",
    "Ce nettoyage permet de réduire le bruit dans les données et de se concentrer sur les mots les plus importants pour l'analyse de sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGavsLOn2SqP"
   },
   "source": [
    "---\n",
    "\n",
    "<10min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "D-6bv3_9Zklz",
    "outputId": "19838845-d8e5-43b6-dced-369f8e1fbc49"
   },
   "outputs": [],
   "source": [
    "#@title Nettoyage des données\n",
    "\n",
    "# Fonction pour nettoyer le texte d'un tweet\n",
    "def clean_tweet(tweet):\n",
    "    # Supprimer les mentions, les hashtags et les liens\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9_]+|#[A-Za-z0-9_]+|http\\S+\", \"\", tweet)\n",
    "    # Supprimer les caractères spéciaux et la ponctuation\n",
    "    tweet = re.sub(r\"[^a-zA-Z ]\", \"\", tweet)\n",
    "    # Convertir le texte en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # Supprimer les mots vides\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweet = \" \".join([word for word in tweet.split() if word not in stop_words])\n",
    "    # Lemmatiser les mots\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweet = \" \".join([lemmatizer.lemmatize(word) for word in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "# Appliquer la fonction de nettoyage à la colonne des tweets\n",
    "df[5] = df[5].apply(clean_tweet)\n",
    "\n",
    "# Affichage des premières lignes du DataFrame avec les tweets nettoyés\n",
    "print(\"Premières lignes du DataFrame avec les tweets nettoyés :\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBIyViGIc2Rf"
   },
   "source": [
    "## 3. Analyse exploratoire\n",
    "\n",
    "Dans cette section, nous réalisons une analyse exploratoire des données pour mieux comprendre les caractéristiques des tweets et identifier des tendances ou des informations importantes.\n",
    "\n",
    "**Les analyses effectuées comprennent :**\n",
    "\n",
    "*   **Distribution des sentiments**: Nous visualisons la distribution des sentiments (positif et négatif) dans le jeu de données pour identifier si les classes sont équilibrées ou non.\n",
    "*   **Fréquence des mots**: Nous calculons la fréquence des mots les plus utilisés dans les tweets pour identifier les termes les plus importants et ceux qui pourraient être caractéristiques de chaque sentiment.\n",
    "*   **Nuage de mots**: Nous générons un nuage de mots pour visualiser les mots les plus fréquents de manière plus intuitive et identifier rapidement les termes les plus importants.\n",
    "\n",
    "Ces analyses nous aident à mieux comprendre les données et à formuler des hypothèses sur les facteurs qui influencent le sentiment exprimé dans les tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "_ZwFEVFSaGH2",
    "outputId": "444ca7fd-c637-46cd-dc66-1465bd1d7bf9"
   },
   "outputs": [],
   "source": [
    "#@title Analyse exploratoire\n",
    "\n",
    "# Analyse de la distribution des sentiments\n",
    "sns.countplot(data=df)\n",
    "plt.title('Distribution des sentiments')\n",
    "plt.show()\n",
    "\n",
    "# Calcul de la fréquence des mots\n",
    "from collections import Counter\n",
    "\n",
    "all_words = ' '.join(df[5].tolist()).split()\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Affichage des 20 mots les plus fréquents\n",
    "print(\"20 mots les plus fréquents :\")\n",
    "print(word_counts.most_common(20))\n",
    "\n",
    "# Création d'un nuage de mots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Nuage de mots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap_9eW_gc_WG"
   },
   "source": [
    "## 4. Préparation des données pour l'entraînement\n",
    "\n",
    "Dans cette section, nous allons préparer les données pour l'entraînement des modèles de Machine Learning.\n",
    "\n",
    "Cela comprend les étapes suivantes :\n",
    "\n",
    "1.  **Encodage des labels** : Convertir les labels de sentiment (0 pour négatif, 4 pour positif) en valeurs numériques (0 et 1).\n",
    "2.  **Division des données** : Diviser les données en ensembles d'entraînement et de test.\n",
    "3.  **Vectorisation du texte** : Transformer le texte des tweets en représentations numériques (vecteurs) en utilisant différentes techniques d'embedding.\n",
    "    *   TF-IDF\n",
    "    *   Word2Vec\n",
    "    *   Doc2Vec\n",
    "4.  **Sauvegarde des données** : Enregistrer les données vectorisées pour chaque technique d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NOM56IXQiA2"
   },
   "outputs": [],
   "source": [
    "#@title Encodage des labels\n",
    "\n",
    "# Encodage des labels de sentiment (0 et 4) en valeurs numériques (0 et 1)\n",
    "df[0] = df[0].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBOPeD6YQmJB"
   },
   "outputs": [],
   "source": [
    "#@title Division des données\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[5], df[0], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEpgpFXhQocp"
   },
   "source": [
    "### Vectorisation avec TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) est une technique de vectorisation qui pondère l'importance des mots en fonction de leur fréquence dans le document et dans l'ensemble du corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niJ66vo52cR9"
   },
   "source": [
    "---\n",
    "\n",
    "t < 1min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiD6JkxWQqFp"
   },
   "outputs": [],
   "source": [
    "#@title Vectorisation TF-IDF\n",
    "\n",
    "# Vectorisation du texte des tweets avec TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_4OKkgXQrb-"
   },
   "source": [
    "### Vectorisation avec Word2Vec\n",
    "\n",
    "Word2Vec est un algorithme qui permet d'apprendre des représentations vectorielles (embeddings) de mots à partir d'un corpus de texte. Chaque mot est représenté par un vecteur dense qui capture sa signification sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvF_zPaY3dal"
   },
   "source": [
    "---\n",
    "\n",
    "t < 5min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KFnrIJOQtJ0"
   },
   "outputs": [],
   "source": [
    "#@title Vectorisation Word2Vec\n",
    "\n",
    "# Entraînement du modèle Word2Vec sur les tweets prétraités\n",
    "sentences = [tweet.split() for tweet in X_train]\n",
    "model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Fonction pour vectoriser un tweet en utilisant la moyenne des embeddings Word2Vec\n",
    "def vectorize_tweet_w2v(tweet):\n",
    "    vectors = [model_w2v.wv[word] for word in tweet.split() if word in model_w2v.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model_w2v.vector_size)\n",
    "\n",
    "# Vectorisation des données d'entraînement et de test\n",
    "X_train_w2v = np.array([vectorize_tweet_w2v(tweet) for tweet in X_train])\n",
    "X_test_w2v = np.array([vectorize_tweet_w2v(tweet) for tweet in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa-VXbbhQuZs"
   },
   "source": [
    "### Vectorisation avec Doc2Vec\n",
    "\n",
    "Doc2Vec est une extension de Word2Vec qui permet d'apprendre des représentations vectorielles (embeddings) de documents entiers, en plus des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QyGfvpWJMQa"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "t < 30min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_i3BbbsUQwCx"
   },
   "outputs": [],
   "source": [
    "#@title Vectorisation Doc2Vec\n",
    "\n",
    "# Préparation des données pour Doc2Vec\n",
    "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(X_train)]\n",
    "\n",
    "# Entraînement du modèle Doc2Vec\n",
    "model_d2v = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Fonction pour vectoriser un tweet en utilisant l'embedding Doc2Vec\n",
    "def vectorize_tweet_d2v(tweet):\n",
    "    return model_d2v.infer_vector(tweet.split())\n",
    "\n",
    "# Vectorisation des données d'entraînement et de test\n",
    "X_train_d2v = np.array([vectorize_tweet_d2v(tweet) for tweet in X_train])\n",
    "X_test_d2v = np.array([vectorize_tweet_d2v(tweet) for tweet in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qp2-iFd7Qx0x"
   },
   "outputs": [],
   "source": [
    "#@title Sauvegarde des données\n",
    "\n",
    "# Chemin d'accès au dossier sur le Drive\n",
    "drive_path = \"/content/drive/My Drive/OC/OC7/\"\n",
    "\n",
    "# Création du dossier \"data\" s'il n'existe pas\n",
    "os.makedirs(drive_path + \"data\", exist_ok=True)\n",
    "\n",
    "# Sauvegarde des données pour chaque méthode\n",
    "def save_data(X_train, X_test, method):\n",
    "    method_path = os.path.join(drive_path, \"data\", method)\n",
    "    os.makedirs(method_path, exist_ok=True)\n",
    "    with open(os.path.join(method_path, 'X_train.pickle'), 'wb') as f:\n",
    "        pickle.dump(X_train, f)\n",
    "    with open(os.path.join(method_path, 'X_test.pickle'), 'wb') as f:\n",
    "        pickle.dump(X_test, f)\n",
    "\n",
    "save_data(X_train_tfidf, X_test_tfidf, \"tfidf\")\n",
    "save_data(X_train_w2v, X_test_w2v, \"word2vec\")\n",
    "save_data(X_train_d2v, X_test_d2v, \"doc2vec\")\n",
    "\n",
    "\n",
    "# Sauvegarde de y_train et y_test dans le dossier principal \"data\"\n",
    "with open(os.path.join(drive_path, \"data\", 'y_train.pickle'), 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open(os.path.join(drive_path, \"data\", 'y_test.pickle'), 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
