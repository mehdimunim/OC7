{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement des données de tweets\n",
    "\n",
    "**Auteur :** Mehdi MUNIM\n",
    "\n",
    "**Date :** 2023-11-26\n",
    "\n",
    "**Description :**\n",
    "\n",
    "Ce notebook prétraite les données de tweets en utilisant les fonctions définies dans le fichier `preprocessing.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 1. Importation des librairies\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Ajouter le chemin du dossier src pour importer les modules\n",
    "sys.path.append('../src')\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 2.1 Chargement des données\n",
    "\n",
    "# Charger les données du fichier CSV\n",
    "df = pd.read_csv('../data/raw/training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "\n",
    "# Renommer les colonnes\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "print(\"Premières lignes du DataFrame :\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 3.1 Nettoyage du texte\n",
    "\n",
    "# Appliquer la fonction de nettoyage du module preprocessing\n",
    "df['text'] = df['text'].apply(preprocessing.clean_tweet)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame avec les tweets nettoyés\n",
    "print(\"Premières lignes du DataFrame avec les tweets nettoyés :\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 3.2 Tokenization\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokeniser les tweets\n",
    "X = tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "y = df['sentiment'].map({0: 0, 4: 1})  # Convertir les labels en 0 et 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 3.3 Vectorisation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split( 1 \n",
    "    df['text'], df['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorisation TF-IDF\n",
    "X_train_tfidf, X_test_tfidf = preprocessing.vectorize_tfidf(X_train, X_test)\n",
    "\n",
    "# Vectorisation Word2Vec\n",
    "X_train_w2v, X_test_w2v = preprocessing.vectorize_word2vec(X_train, X_test)\n",
    "\n",
    "# Vectorisation Doc2Vec\n",
    "X_train_d2v, X_test_d2v = preprocessing.vectorize_doc2vec(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sauvegarde des données prétraitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 4.1 Sauvegarde des données\n",
    "\n",
    "import os\n",
    "\n",
    "# Chemin d'accès au dossier data\n",
    "data_path = '../data/processed/'\n",
    "\n",
    "# Créer le dossier s'il n'existe pas\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Sauvegarde des données pour chaque méthode\n",
    "def save_data(X_train, X_test, methode):\n",
    "    with open(data_path + f'X_train_{methode}.pickle', 'wb') as f:\n",
    "        pickle.dump(X_train, f)\n",
    "    with open(data_path + f'X_test_{methode}.pickle', 'wb') as f:\n",
    "        pickle.dump(X_test, f)\n",
    "\n",
    "save_data(X_train_tfidf, X_test_tfidf, \"tfidf\")\n",
    "save_data(X_train_w2v, X_test_w2v, \"word2vec\")\n",
    "save_data(X_train_d2v, X_test_d2v, \"doc2vec\")\n",
    "\n",
    "# Sauvegarde de y_train et y_test\n",
    "with open(data_path + 'y_train.pickle', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open(data_path + 'y_test.pickle', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
