{"cells":[{"cell_type":"markdown","metadata":{"id":"il5Rr1nMLQ8Z"},"source":["# Fine-tuning de BERT pour l'analyse de sentiment\n","\n","**Auteur :** MUNIM Mehdi\n","\n","**Date :** 2023-11-18\n","\n","**Description :**\n","\n","Ce notebook explore le fine-tuning de modèles BERT pré-entraînés pour la classification de sentiment sur des tweets.\n","\n","**Objectif :**\n","\n","L'objectif est de fine-tuner un modèle BERT pour prédire avec précision le sentiment (positif ou négatif) exprimé dans des tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23381,"status":"ok","timestamp":1732015976459,"user":{"displayName":"Mehdi Munim","userId":"16889469231430174501"},"user_tz":-60},"id":"XKjEX5rgK6ns","outputId":"ac328a0a-d4c4-4d1a-d60e-8b9e7cc90b59"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["#@title Imports\n","\n","# Librairies pour la manipulation et l'analyse des données\n","import pandas as pd\n","import numpy as np\n","\n","# Librairies pour le traitement du langage naturel (NLP)\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import re\n","\n","# Librairies pour la visualisation\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Librairies pour le Machine Learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Librairies pour les embeddings\n","from gensim.models import Word2Vec, Doc2Vec\n","\n","# Librairies pour la sauvegarde des données\n","import os\n","import pickle\n","\n","# Librairies pour BERT\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification, AutoTokenizer, TFAutoModelForSequenceClassification\n","\n","# Téléchargement des ressources NLTK\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{"id":"GIuElO9MMtoI"},"source":["## 1. Téléchargement et préparation des données\n","\n","Dans cette section, nous allons télécharger les données, les charger dans un DataFrame pandas, puis les prétraiter et les tokenizer pour les utiliser avec les modèles BERT."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84564,"status":"ok","timestamp":1732016061018,"user":{"displayName":"Mehdi Munim","userId":"16889469231430174501"},"user_tz":-60},"id":"XyTvsA_mMYO6","outputId":"97038ab3-0312-4761-cdc9-5b681e902113"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-11-19 11:32:56--  https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip\n","Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.105.242, 52.218.57.3, 52.92.1.112, ...\n","Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.105.242|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84855679 (81M) [application/zip]\n","Saving to: ‘sentiment140.zip.2’\n","\n","sentiment140.zip.2  100%[===================>]  80.92M  8.82MB/s    in 21s     \n","\n","2024-11-19 11:33:17 (3.94 MB/s) - ‘sentiment140.zip.2’ saved [84855679/84855679]\n","\n","Archive:  sentiment140.zip\n","replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: training.1600000.processed.noemoticon.csv  y\n","\n"]}],"source":["#@title 1.1 Téléchargement des données\n","\n","!wget \"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/AI+Engineer/Project+7%C2%A0-+D%C3%A9tectez+les+Bad+Buzz+gr%C3%A2ce+au+Deep+Learning/sentiment140.zip\"\n","!unzip sentiment140.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qmacWGmZMxQo"},"outputs":[],"source":["#@title 1.2 Chargement et prétraitement des données\n","\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import re\n","\n","# Charger les données du fichier CSV\n","df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n","\n","# Fonction pour nettoyer le texte d'un tweet\n","def clean_tweet(tweet):\n","    # Supprimer les mentions, les hashtags et les liens\n","    tweet = re.sub(r\"@[A-Za-z0-9_]+|#[A-Za-z0-9_]+|http\\S+\", \"\", tweet)\n","    # Supprimer les caractères spéciaux et la ponctuation\n","    tweet = re.sub(r\"[^a-zA-Z ]\", \"\", tweet)\n","    # Convertir le texte en minuscules\n","    tweet = tweet.lower()\n","    # Supprimer les mots vides\n","    stop_words = set(stopwords.words('english'))\n","    tweet = \" \".join([word for word in tweet.split() if word not in stop_words])\n","    # Lemmatiser les mots\n","    lemmatizer = WordNetLemmatizer()\n","    tweet = \" \".join([lemmatizer.lemmatize(word) for word in tweet.split()])\n","    return tweet\n","\n","# Appliquer la fonction de nettoyage à la colonne des tweets\n","df[5] = df[5].apply(clean_tweet)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AcKJ0a0fMbJK"},"outputs":[],"source":["#@title 1.2' Échantillonage\n","\n","# Échantillonner 1% des données\n","df = df.sample(frac=0.01, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6v6BIQ9MznF"},"outputs":[],"source":["#@title 1.3 Tokenization des données\n","\n","from transformers import AutoTokenizer\n","\n","# Charger le tokenizer BERT\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokeniser les tweets\n","X = tokenizer(\n","    df[5].tolist(),\n","    padding='max_length',\n","    max_length=50,\n","    truncation=True,\n","    return_tensors=\"tf\"\n",")\n","y = df[0].map({0: 0, 4: 1})  # Convertir les labels en 0 et 1"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"temOWOQVM1HA","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"error","timestamp":1732019250248,"user_tz":-60,"elapsed":312,"user":{"displayName":"Mehdi Munim","userId":"16889469231430174501"}},"outputId":"d8e4685c-d32b-4cc4-bf4c-27a957b951dc"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([ 9094,  4284,   622, ..., 11908, 11743, 11335])","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8f5f88f905f5>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mX_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mX_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2811\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2812\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     return list(\n\u001b[1;32m   2811\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2812\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2813\u001b[0m         )\n\u001b[1;32m   2814\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_polars_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/tensor_getitem_override.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([ 9094,  4284,   622, ..., 11908, 11743, 11335])"]}],"source":["#@title 1.4 Division en ensembles d'entraînement et de test\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Diviser les données en ensembles d'entraînement et de test\n","(\n","    input_ids_train,\n","    input_ids_test,\n","    attention_mask_train,\n","    attention_mask_test,\n","    token_type_ids_train,\n","    token_type_ids_test,\n","    labels_train,\n","    labels_test,\n",") = train_test_split(\n","    X[\"input_ids\"],\n","    X[\"attention_mask\"],\n","    X[\"token_type_ids\"],\n","    y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42,\n",")"]},{"cell_type":"markdown","metadata":{"id":"b3iiISheNMcB"},"source":["## 2. Fine-tuning du modèle BERT\n","\n","Dans cette section, nous allons charger un modèle BERT pré-entraîné, définir les hyperparamètres d'entraînement, puis fine-tuner le modèle sur les données d'entraînement.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFYsf6NTNMnh"},"outputs":[],"source":["#@title 2.1 Chargement du modèle BERT\n","\n","from transformers import TFAutoModelForSequenceClassification\n","\n","# Charger le modèle BERT pré-entraîné\n","model_name = \"bert-base-uncased\"  # Nom du modèle BERT à utiliser\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QrTargfNNx4"},"outputs":[],"source":["#@title 2.2 Définition des hyperparamètres\n","\n","# Définir les hyperparamètres du modèle\n","learning_rate = 2e-5\n","epochs = 3\n","batch_size = 8  # Réduire la taille des batchs pour économiser de la RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlgAenLJNPUs"},"outputs":[],"source":["#@title 2.3 Entraînement du modèle\n","\n","# Compiler le modèle\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"],\n",")\n","\n","# Entraîner le modèle\n","history = model.fit(\n","    x=[input_ids_train, attention_mask_train, token_type_ids_train],\n","    y=labels_train,\n","    validation_split=0.2,\n","    epochs=epochs,\n","    batch_size=batch_size,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqKFkOEnNQnL"},"outputs":[],"source":["#@title 2.4 Évaluation du modèle\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    confusion_matrix,\n",")\n","\n","# Fonction pour afficher les métriques d'évaluation\n","def evaluer_modele(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1-score: {f1:.4f}\")\n","\n","    # Afficher la matrice de confusion\n","    plt.figure(figsize=(5, 5))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n","    plt.title(\"Matrice de confusion\")\n","    plt.ylabel(\"Vraie classe\")\n","    plt.xlabel(\"Classe prédite\")\n","    plt.show()\n","\n","# Prédictions du modèle\n","y_pred = model.predict(\n","    [input_ids_test, attention_mask_test, token_type_ids_test]\n",").logits\n","y_pred = np.argmax(y_pred, axis=1)\n","\n","# Évaluation du modèle\n","evaluer_modele(labels_test, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"az0lf2HANm7x"},"source":["## 3. Analyse des résultats\n","\n","Dans cette section, nous allons analyser les performances du modèle fine-tuné en utilisant différentes métriques et visualisations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9fB2y2aNnMh"},"outputs":[],"source":["#@title 3.1 Affichage des métriques\n","\n","# Prédictions du modèle\n","y_pred = model.predict(\n","    [input_ids_test, attention_mask_test, token_type_ids_test]\n",").logits\n","y_pred = np.argmax(y_pred, axis=1)\n","\n","# Évaluation du modèle\n","evaluer_modele(labels_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCgrqH8xNrJe"},"outputs":[],"source":["#@title 3.2 Analyse des erreurs\n","\n","# Afficher quelques exemples de tweets mal classés\n","for i in range(len(labels_test)):\n","    if labels_test[i] != y_pred[i]:\n","        print(f\"Tweet : {df[5][i]}\")\n","        print(f\"Vraie classe : {labels_test[i]}\")\n","        print(f\"Classe prédite : {y_pred[i]}\")\n","        print(\"-\" * 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53KQg8dgNyWK"},"outputs":[],"source":["#@title 3.3 Visualisation des résultats\n","\n","# Afficher la courbe d'apprentissage\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Accuracy du modèle')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Entraînement', 'Validation'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Loss du modèle')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Entraînement', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wRbwXhK3ODyp"},"source":["## 4. Sauvegarde du modèle fine-tuné\n","\n","Dans cette section, nous allons sauvegarder le modèle BERT fine-tuné pour une utilisation ultérieure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TONjUQ4jOGyI"},"outputs":[],"source":["#@title 4.1 Sauvegarde du modèle fine-tuné\n","\n","# Sauvegarder le modèle fine-tuné\n","model.save_pretrained(\"mon_modele_bert_finetuned\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOibZ94SjqQ9AGFLu2Ni7Av"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}